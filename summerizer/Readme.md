# Text Summerizer

### Tokenising
<img width="1593" alt="Screenshot 2024-01-27 at 7 28 08 PM" src="https://github.com/AlgoRexx/NLP_projects/assets/146161841/9299a979-222a-4bbc-b65a-7e378ae3e36e">

1. <code>Advanced Transformer Model Deployment:</code>
   - The implementation marks a notable milestone in natural language processing (NLP) by deploying an efficient Transformer model. The model showcases sophisticated attention mechanisms, including Dot Product attention, casual attention, and self-attention. This demonstrates a deep understanding of attention mechanisms, enhancing the model's capacity to capture intricate contextual dependencies.

2. <code>Nuanced Attention Mechanisms:</code>
   - The utilization of advanced attention mechanisms, such as Dot Product attention and casual attention, signifies a nuanced approach. These mechanisms play a crucial role in enhancing the model's ability to focus on relevant parts of the input sequence, contributing significantly to its overall effectiveness in understanding and processing contextual information.

3. <code>Transformer Language Model for Decoding:</code>
   - The integration of a Transformer Language Model (TransformerLM) for decoding represents a robust choice. This strategic incorporation underscores a comprehensive approach to sequence generation, leveraging the Transformer architecture's capabilities in handling sequential data. The TransformerLM enhances the model's proficiency in generating coherent and contextually relevant sequences.

4. <code>Greedy Decoding Strategy:</code>
   - The implementation adopts Greedy Decoding as a strategy for sequence generation. This approach complements the Transformer architecture, emphasizing efficiency and simplicity in selecting the most probable token at each decoding step. The combination of a powerful Transformer model with Greedy Decoding enhances the overall performance of the system in generating high-quality sequences.

### Using <SOS> and <EOS> tokens
<img width="1418" alt="Screenshot 2024-01-27 at 7 31 27 PM" src="https://github.com/AlgoRexx/NLP_projects/assets/146161841/ac3e7bae-c66c-420f-9fe6-dbd6e5c6f665">

### Implimentation
https://drive.google.com/file/d/1RrpviKf0I2t9FDoDDp6Yo-3alK1KnRSJ/view?usp=sharing
